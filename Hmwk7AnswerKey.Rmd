---
title: "Homework 7 Answer Key"
author: "Melinda K. Higgins, PhD."
date: "April 6, 2017"
output:
  word_document: default
  html_document: default
  pdf_document:
    keep_tex: yes
  github_document: default
---

```{r setup, include=FALSE}
# leave echo = TRUE to see code
knitr::opts_chunk$set(echo = TRUE)

# but suppress messages and warnings
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

-----

## Homework 7 - Assignment

Recall the NHANES dataset that we used in Lesson 10. 

1. In the dataset there is a discrete variable called SleepTrouble indicating whether each participant has trouble sleeping or not. You are going to build a set of classifiers for this dependent variable. You may use any (set of) independent variable(s) you like except for the variable callsed SleepHrsNight. 
For each of the model types **(null model, logistic regression, decision tree, random forest, k-nearest neighbor)** do the following:

    1A. Build the classifier.

    1B. Report its effectiveness on the NHANES dataset.

    1C. Make an appropriate visualization of this model.

    1D. Interpret the results. What have you learned about people's sleeping habits?

2. Repeat problem 1 except now you are to use the quantitative variable called SleepHrsNight. The model types are as follows: null model, multiple regression, regression tree, random forest.

-----

## Homework 7 - Answer Key

Given the instructions providing in the assignment (above), it is useful to review the code and examples provided in "lesson 10" in Dr. Hertzberg's Github repository at [https://github.com/vhertzb/Lesson10](https://github.com/vhertzb/Lesson10). Specifically, review the steps in the `Lesson10.Rmd` R markdown file. This "part 1" covered the code and steps for predicting `Diabetes` from these variables: `Age`, `Gender`, `BMI`, `HHIncome`, and `PhysActive`. The code and examples shown include:

* the "null" model, which is the model for the outcome with NO predictors in the model. The model only has the intercept, so the model is of the form: $Y = \beta_0 + \epsilon$
    - for a "logistic regression" approach, the NULL model is basically the proportion of people with the outcome of interest (e.g. Diabetes or Sleep Trouble) versus not
    - for a "linear regression" approach, the NULL model is the mean (average) level across all subjects for the numerical/continuous outcome measure.
* a "regression-type" model: 
    - logistic regression approach for a binary/categorical outcome
    - linear regression approach for a continuous/numerical outcome
* running KNN (k-Nearest Neighbor Classification) (via `knn` from the `class` package)
* a decision tree (via `rpart()` from the `rpart` package)
* and using `randomForest` from the `randomForest` package

**NOTE**: Lesson 10's `Lesson10.Rmd` R markdown did not include the code for running a logistic regression for Diabetes nor a linear regression example, but these topics were covered earlier during weeks 5 and 6; 

* see [https://github.com/melindahiggins2000/N741linearlogmodels](https://github.com/melindahiggins2000/N741linearlogmodels) when we covered generalized linear models;
* also see [https://github.com/vhertzb/Regression-1](https://github.com/vhertzb/Regression-1);
* we also did more with logistic regression in week 6, see [https://github.com/melindahiggins2000/N741predict](https://github.com/melindahiggins2000/N741predict);
* and [https://melindahiggins2000.github.io/N741bigdata/models.html](https://melindahiggins2000.github.io/N741bigdata/models.html); 
* and [https://melindahiggins2000.github.io/N741bigdata/prediction.html](https://melindahiggins2000.github.io/N741bigdata/prediction.html).

### Load the NHANES dataset and review the variables included

It is ALWAYS a good idea to make sure you review the variables included and think about how they were recorded, what type of data each variable is (categorical, ordinal, numeric, continuous, etc), and the distributions of each.

```{r}
# load the NHANES package with the NHANES dataset
library(NHANES)

# create a data object for the NHANES dataset
dat1 <- NHANES

# list all of the variables included
names(dat1)

# other packages needed
library(dplyr)
library(ggplot2)
```

### Investigate the 2 Outcomes of Interest `SleepTrouble` and `SleepHrsNight`

```{r}
class(dat1$SleepTrouble)
summary(dat1$SleepTrouble)

class(dat1$SleepHrsNight)
summary(dat1$SleepHrsNight)
```

So, `SleepTrouble` is a "Factor" with 2 levels "No" and "Yes" with some missing data "NA"s.

But `SleepHrsNight` is a numeric variable (specifically an integer) with values ranging from 2 to 12 hours per night.

### Visualize `SleepTrouble` Outcome of Interest

NOTE: `ggplot()` codes based on examples at "Cookbook for R" website for the "R Graphics Cookbook" book at [http://www.cookbook-r.com/Graphs/](http://www.cookbook-r.com/Graphs/).

```{r}
# Bar chart of Frequency/Counts for Sleep Trouble with NAs
dat1 %>%
  ggplot(aes(x=SleepTrouble, fill=SleepTrouble)) +
    geom_bar(stat="count", colour="black") +
    ggtitle("Frequency of Subjects with Sleep Trouble")
```

### You'll notice the following for `SleepTrouble`:

1. It is a factor with 2 levels with values of "No and "Yes", which we can see by running `head(dat1$SleepTrouble)`. It is important to note this since some "classifier" procedures and functions in R assume that the "target" variable is coded 0 or 1.

```{r}
head(dat1$SleepTrouble)
```

2. The majority of the subjects do NOT have Sleep Trouble - most are "No"s
3. There are also a decent number of NAs which will be removed in the final analyses - or at least ignored. It will be important to know how the chosen classifier function or procedure handles missing NA data.

### Visualize `SleepHrsNight` Outcome of Interest

```{r}
# Histogram overlaid with kernel density curve
dat1 %>% 
  ggplot(aes(x=SleepHrsNight)) + 
    geom_histogram(aes(y=..density..), 
                   binwidth=1,
                   colour="black", fill="yellow") +
    geom_density(alpha=.2, fill="blue", adjust=2) +
    ggtitle("Histogram Density Plot of Sleep Hours Per Night")
```

### You'll notice the following for `SleepHrsNight`:

1. There were still quite a few missing values as seen when we ran the `summary(dat1$SleepHrsNight)` above. It will be important to know how the chosen classifier function or procedure handles missing NA data.
2. Since this is a numeric outcome, the classifiers chosen will be performing "regression" based models as opposed to "category probability" type models like logistic regression.
3. That said, the distribution of `SleepHrsNight` is reasonably symmetric and approximately normally distributed which is good with no obvious outliers, even though the range of sleep times is wide from 2 to 12 hours, which is interesting.

## Overall Notes on **Open-Ended Approach** for This Homework 7 Assignment

While the NHANES dataset had 76 variables, it was intended for you to choose a subset you thought would be appropriate for predicting/classifying those with trouble sleeping and/or their time spent sleeping.

A side note on sleep times - given that the hours spent sleeping was numeric and ordinal in nature and was symmetrically and approximately normally distributed, it was OK to approach modeling `SleepHrsNight` as a continuous/numeric outcome (i.e. a "regression-type" approach). However, it was also OK if you decided to recode this variable into subjects with low sleep times (say < 7) versus more sleep. You could have also looked at recoding `SleephrsNight` into those with optimal sleep (7-9 hrs) versus less than optimal (which includes both those people with less than 7 hrs and more than 9 hrs - both too little and too much sleep can be problematic). Any of these approaches were OK.

## PART 1 - Build "classifiers" for `SleepTrouble`

For each of the model types (null model, logistic regression, decision tree, random forest, k-nearest neighbor):

* 1A. Build the classifier.
* 1B. Report its effectiveness on the NHANES dataset.
* 1C. Make an appropriate visualization of this model.
* 1D. Interpret the results. What have you learned about people's sleeping habits?

### Pick a subset of likely variables for predicting Sleep Trouble

For my approach, I choose the following 10 variables as possible predictors of sleep problems:

1. age
2. gender
3. marital status
4. poverty
5. home ownership
6. BMI
7. Diabetes
8. health in general
9. depressed
10. physically active

```{r}
# build dataset for these 10 variables and SleepTrouble
people <- NHANES %>% 
  select(Age, Gender, MaritalStatus, Poverty, HomeOwn,
         BMI, Diabetes, HealthGen, Depressed, PhysActive,
         SleepTrouble) 

# run summary
summary(people)

# Convert back to dataframe
people <- as.data.frame(people)
glimpse(people)

# Convert factors to numeric - the packages just seem to work better that way
people$Gender <- as.numeric(people$Gender)
people$MaritalStatus <- as.numeric(people$MaritalStatus)
people$HomeOwn <- as.numeric(people$HomeOwn)
people$Diabetes <- as.numeric(people$Diabetes)
people$HealthGen <- as.numeric(people$HealthGen)
people$Depressed <- as.numeric(people$Depressed)
people$PhysActive <- as.numeric(people$PhysActive)
people$SleepTrouble <- as.numeric(people$SleepTrouble)

summary(people)
dim(people)

# drop any cases/rows with missing data
# this step creates a complete cases dataset
people <- na.omit(people)
summary(people)
dim(people)
```

-----

#### SUGGESTION TO REMEMBER

**NOTE:** The last code step above drops any case with missing data. This was done to avoid problems with some of these procedures which vary in how they handle missing data. In general, missing data issues should be addressed BEFORE any analyses are performed since most (nearly all) assume complete cases. So, REMEMBER to always review and discuss how missing data was addressed in your own studies and analyses BEFORE finalizing your statistical models and tests.

-----

### Run Logistic Regression - predict Sleep Trouble

```{r}
# recode SleepTrouble into 0 (for no sleep trouble)
# and 1 (for yes Sleep Trouble); so we need to recode
# values of 2 (which were yes) to 1. We'll use
# the double equals to find all TRUE values and then convert
# the logical results into 0's and 1's.
people$SleepTrouble <- as.numeric(people$SleepTrouble==2)

# model SleepTrouble by rest of variables in people dataset
fmla <- "SleepTrouble ~ ."

# NOTE: This will result in a model of the form
# SleepTrouble ~ Age + Gender + MaritalStatus +
#                Poverty + HomeOwn + BMI + Diabetes +
#                HealthGen + Depressed + PhysActive

logreg <- glm(fmla, 
              data=people, 
              family=binomial(link="logit"))

summary(logreg)

class(logreg)

# Get the predictions - the predicted probabilities
# of SleepTouble Yes for each case
# 
# NOTE: the function predict() is a generic function
# which recognizes the "logreg" output from the logistic regression
# and in reality the function really calls predict.glm
# since logreg is of class glm.
people$pred <- predict(logreg, 
                       newdata=people, 
                       type="response")

# plot predicted probabilities
ggplot(people, 
       aes(x=pred, color=as.factor(SleepTrouble), 
           linetype=as.factor(SleepTrouble))) +
  geom_density() +
  ggtitle("Predicted Probability for Sleep Trouble")

# pick a threshold and get confusion (prediction) matrix
# test a classifier with a threshold > 0.20
ctab <- table(pred=people$pred>0.2, SleepTrouble=people$SleepTrouble)
ctab

# compute precision = true positives / predicted true
precision <- ctab[2,2]/sum(ctab[2,])
precision

# compute recall = true positives / actual true
recall <- ctab[2,2]/sum(ctab[,2])
recall

# look at ROC curve
library(pROC)
roccurve <- roc(people$SleepTrouble ~ people$pred)
plot(roccurve)

# pull out just the AUC statistic
auc(roccurve)
```

### NULL MODEL for Logistic Regression

We didn't cover this in class, but to get an "intercept-only" model you use a formula in the form of `outcome ~ 1` which basically says model the `outcome` variable as a function of the intercept indicated by the `1`. The `1` is used since the intercept term $\beta_0$ is implicitly multiplied by `1`. The function is of the form $Y = \beta_0*(1) + \epsilon$.

This formula can be used for any generalized linear modeling approach (linear regression, logistic regression, Poisson regression, etc). You'll notice in running the code steps below that using the intercept only approach does no better than flipping a coin which you see for the ROC curve which is a straight line and the AUC is 0.5 (50/50 guessing does as well as this null model with no predictors). You always want the AUC to be >0.5 and as close to 1.0 as possible. AUCs >0.7 are ok but you really want AUCs >0.8 and >0.9 is even better.

```{r}

# NULL MODEL for Logistic Regression 
# is basically an intercept-only model with no predictors
logreg.null <- glm(SleepTrouble ~ 1, 
                   data=people, 
                   family=binomial(link="logit"))

summary(logreg.null)

people$pred.null <- predict(logreg.null, 
                       newdata=people, 
                       type="response")

# plot predicted probabilities
ggplot(people, 
       aes(x=pred.null, color=as.factor(SleepTrouble), 
           linetype=as.factor(SleepTrouble))) +
  geom_density() +
  ggtitle("Predicted Probability for Sleep Trouble - Null Model")

# pick a threshold and get confusion (prediction) matrix
# test a classifier with a threshold > 0.30
ctab <- table(pred=people$pred>0.3, SleepTrouble=people$SleepTrouble)
ctab

# compute precision = true positives / predicted true
precision <- ctab[2,2]/sum(ctab[2,])
precision

# compute recall = true positives / actual true
recall <- ctab[2,2]/sum(ctab[,2])
recall

# look at ROC curve
#library(pROC)
roccurve <- roc(people$SleepTrouble ~ people$pred.null)
plot(roccurve)

# pull out just the AUC statistic
auc(roccurve)
```

### Try KNN to predict Sleep Trouble

```{r}
# for knn, rpart and randomForest, set SleepTrouble
# back to being a factor
people$SleepTrouble <- as.factor(people$SleepTrouble)

# Apply knn procedure to predict Diabetes
# use the knn procedure in the class package
library(class)

# Let's try different values of k to see how that affects performance
knn.1 <- knn(train = people, test = people, cl = people$SleepTrouble, k = 1)
knn.3 <- knn(train = people, test = people, cl = people$SleepTrouble, k = 3)
knn.5 <- knn(train = people, test = people, cl = people$SleepTrouble, k = 5)
knn.20 <- knn(train = people, test = people, cl = people$SleepTrouble, k = 20)

# see how well they classified
# Calculate the percent predicted correctly

100*sum(people$SleepTrouble == knn.1)/length(knn.1)
100*sum(people$SleepTrouble == knn.3)/length(knn.3)
100*sum(people$SleepTrouble == knn.5)/length(knn.5)
100*sum(people$SleepTrouble == knn.20)/length(knn.20)

#overall success
# Another way to look at success rate against increasing k

table(knn.1, people$SleepTrouble)
table(knn.3, people$SleepTrouble)
table(knn.5, people$SleepTrouble)
table(knn.20, people$SleepTrouble)
```

### (Version 1) The ensemble method - using the approach from lesson 10 with Age and BMI

The example here uses the same basic code we did in class for lesson 10. This looks at just Age and BMI and no other variables considered in the decision tree and random forest models. The plots also just consider Age and BMI.

```{r}
library(mosaic)
# Create the grid
ages <- mosaic::range(~ Age, data = people)
bmis <- mosaic::range(~ BMI, data = people)
res <- 100
fake_grid <- expand.grid(
  Age = seq(from = ages[1], to = ages[2], length.out = res),
  BMI = seq(from = bmis[1], to = bmis[2], length.out = res))

#Get the overall proportion, p, of people with Sleep Trouble
p <- sum(people$SleepTrouble == 1)/length(people$SleepTrouble)
p

# Null model prediction
pred_null <- rep(p, nrow(fake_grid))

# model with only Age and BMI considered
form <- as.formula("SleepTrouble ~ Age + BMI")

library(rpart)
# Evaluate each model on each grid point
# For the decision tree
dmod_tree <- rpart(form, data = people, 
                   control = rpart.control(cp = 0.005, minbucket = 30))

# results summary
dmod_tree

# For random forest
set.seed(20371)
library(randomForest)
dmod_forest <- randomForest(form, data = people, 
                     ntree = 201, mtry = 2)

# results summary
dmod_forest

# Now the predictions for tree and forest
# REMEMBER predict() is generic and will execute
# the correct predict() function based on the class
# of model object we give it.
class(dmod_tree)

# dmod_tree is an object of "rpart" class, so
# when we call predict() we are really calling
# the predict.rpart() function
# from the function below, we keep column 2
# which is predicting SleepTrouble=1 or YES
pred_tree <- predict(dmod_tree, newdata = fake_grid)[,2]
summary(pred_tree)

# The analogous approach is used for predicting
# results using the random forest model
# so predict() calls predict.randomForest() since
# dmod_forest is of "randomForest" class
class(dmod_forest)
pred_forest <- predict(dmod_forest, newdata = fake_grid, type = "prob")[,2]
summary(pred_forest)

# predicting SleepTrouble across the Age, BMI grid
# fake data we created above - the predictions
# are based on the model where k=5
# K-nearest neighbor prediction
pred_knn <- people %>%
  select(Age, BMI) %>%
  knn(test=select(fake_grid, Age, BMI), cl = people$SleepTrouble, k=5) %>%
  as.numeric() - 1

```

Next, we want to build a dataframe with all of these predicted models, then `gather()` it into a long format.

```{r}
library(tidyr)

# build the data frame
res <- fake_grid %>%
  mutate(
    "Null" = pred_null, 
    "Decision Tree" = pred_tree,
    "Random Forest" = pred_forest, 
    "K-nearest neighbor" = pred_knn) %>%
  gather(k="model", value = "y_hat", -Age, -BMI)

```

Next let's plot all of these

```{r}

ggplot(data = res, aes(x = Age, y = BMI)) +
  geom_tile(aes(fill=y_hat), color = NA) +
  geom_count(aes(color = SleepTrouble), alpha = 0.4, data = people) +
  scale_fill_gradient(low = "white", high = "blue") +
  scale_color_manual(values = c("gray", "gold")) +
  scale_size(range = c(0,2)) +
  scale_x_continuous(expand = c(0.02, 0)) +
  scale_y_continuous(expand = c(0.02, 0)) +
  facet_wrap(~model)

```


### (Version 2): The ensemble method - using all 10 variables I choose

The decision tree (`rpart`) and random forest models below use the 10 variables I selected. I then modified the plots to look at each models predictions based on the original data (i.e. _**I did NOT generate fake-data below**_). However, the plots show the predictions over the available data for Age and Poverty instead of Age and BMI. I also added plots to show the decision tree and the variable importance plot from random forest so you can see that indeed BMI, Age and Poverty are indeed important in predicting sleep trouble.

```{r}
people <- people %>%
  select(Age, Gender, MaritalStatus, Poverty,
         HomeOwn, BMI, Diabetes, HealthGen, Depressed,
         PhysActive, SleepTrouble)
  
# note: all variables are numeric now, except
# SleepTrouble was a factor, but is now numeric
# and coded 0 for no and 1 for yes

#Get the overall proportion, p, of people with Sleep Trouble
p <- sum(people$SleepTrouble == 1)/length(people$SleepTrouble)
p

# Null model prediction
pred_null <- rep(p, nrow(people))

form <- as.formula("SleepTrouble ~ .")

dmod_tree <- rpart(form, data = people, 
                   control = rpart.control(cp = 0.005, minbucket = 30))

# results summary
dmod_tree

# draw the tree - see example
# in the help at help(plot.rpart)
par(xpd = TRUE)
plot(dmod_tree, compress = TRUE)
text(dmod_tree, use.n = TRUE)
# age and BMI are near the top, but 
# so is Depressed and Poverty

# For random forest
set.seed(20371)
dmod_forest <- randomForest(form, data = people, 
                     ntree = 201, mtry = 2)

# results summary
dmod_forest
varImpPlot(dmod_forest)
# you'll notice that BMI and Age are at the top
# of this Variable Important plot
# also near the top is the Poverty level

# Now the predictions for tree and forest
# just compute prediction from original data for now
# again keep column 2 for SleepTrouble=1 YES
pred_tree <- predict(dmod_tree)[,2]
summary(pred_tree)

pred_forest <- predict(dmod_forest, type = "prob")[,2]
summary(pred_forest)

# K-nearest neighbor prediction
# but look at Age and Poverty
pred_knn <- people %>%
  knn(test=people, cl = people$SleepTrouble, k=5) %>%
  as.numeric() - 1
```

Next, we want to build a dataframe with all of these predicted models, then `gather()` it into a long format.

```{r}
# build the data frame
res <- people %>%
  mutate(
    "Null" = pred_null, 
    "Decision Tree" = pred_tree,
    "Random Forest" = pred_forest, 
    "K-nearest neighbor" = pred_knn) %>%
  gather(k="model", value = "y_hat", -Age, -Gender,
         -MaritalStatus, -Poverty, -HomeOwn, -BMI,
         -Diabetes, -HealthGen, -Depressed, -PhysActive,
         -SleepTrouble)
```

Next let's plot all of these. These plots are not as interesting as the ones above since the predictions are only done at points for which we had those Ages and Poverty levels - we did NOT build a complete fake grid...

```{r}
ggplot(data = res, aes(x = Age, y = Poverty)) +
  geom_tile(aes(fill=y_hat), color = NA) +
  geom_count(aes(color = SleepTrouble), alpha = 0.4, data = people) +
  scale_fill_gradient(low = "white", high = "blue") +
  scale_color_manual(values = c("gray", "gold")) +
  scale_size(range = c(0,2)) +
  scale_x_continuous(expand = c(0.02, 0)) +
  scale_y_continuous(expand = c(0.02, 0)) +
  facet_wrap(~model)
```

-----

## PART 2 - Build "classifiers" for `SleepHrsNight`

For each of the model types (null model, logistic regression, decision tree, random forest, k-nearest neighbor):

* 1A. Build the classifier.
* 1B. Report its effectiveness on the NHANES dataset.
* 1C. Make an appropriate visualization of this model.
* 1D. Interpret the results. What have you learned about people's sleeping habits?

### Suppose we recode `SleepHrsNight` into optimal and suboptimal sleep

To use the code based on what we did above, it'll be best to create a categorical.binary outcome. Let's suppose that if the person got 7-9 hours of sleep each night that is optimal sleep time, but any sleep times less than 7 hours or more than 9 hours is considered suboptimal. 

_**NOTE: Keep this issue in mind when it comes to sleep times, just because we have Sleep Hours per Night as a numeric/somewhat continuous variable, too much can be bad just as too little, so this outcome probably has a non-linear and probably somewhat quadratic type of response, so a linear approach to this outcome might not be the best anyway. Another idea is to break this outcome into 3 categories or "classes" for subjects with too little sleep, just right and too much sleep - each may have different clinical implications.**_

I'll use the same approach as above for a new variable created below `SleepOptimal` and same 10 selected variables subset.

```{r}
# build dataset for these 10 variables and SleepHrsNight
people <- NHANES %>% 
  select(Age, Gender, MaritalStatus, Poverty, HomeOwn,
         BMI, Diabetes, HealthGen, Depressed, PhysActive,
         SleepHrsNight) 

# run summary
summary(people)

# Convert back to dataframe
people <- as.data.frame(people)
glimpse(people)

# Convert factors to numeric - the packages just seem to work better that way
people$Gender <- as.numeric(people$Gender)
people$MaritalStatus <- as.numeric(people$MaritalStatus)
people$HomeOwn <- as.numeric(people$HomeOwn)
people$Diabetes <- as.numeric(people$Diabetes)
people$HealthGen <- as.numeric(people$HealthGen)
people$Depressed <- as.numeric(people$Depressed)
people$PhysActive <- as.numeric(people$PhysActive)
people$SleepHrsNight <- as.numeric(people$SleepHrsNight)

summary(people)
dim(people)

# drop any cases/rows with missing data
# this step creates a complete cases dataset
people <- na.omit(people)
summary(people)
dim(people)

# recode into optimal and suboptimal sleep times
# if hours is between 7 and 9 set outcome to 1, else set to 0
people$SleepOptimal <- ifelse((people$SleepHrsNight <= 9 &
                                 people$SleepHrsNight >= 7), 1, 0)
```

### Run Logistic Regression - predict Sleep Trouble

```{r}
# drop SleepHrsNight
people <- people %>% select(-SleepHrsNight)

# model SleepTrouble by rest of variables in people dataset
fmla <- "SleepOptimal ~ ."

logreg <- glm(fmla, 
              data=people, 
              family=binomial(link="logit"))

summary(logreg)

people$pred <- predict(logreg, 
                       newdata=people, 
                       type="response")

# plot predicted probabilities
ggplot(people, 
       aes(x=pred, color=as.factor(SleepOptimal), 
           linetype=as.factor(SleepOptimal))) +
  geom_density() +
  ggtitle("Predicted Probability for Sleep Optimal")

# pick a threshold and get confusion (prediction) matrix
# test a classifier with a threshold > 0.70
ctab <- table(pred=people$pred>0.7, SleepOptimal=people$SleepOptimal)
ctab

# compute precision = true positives / predicted true
precision <- ctab[2,2]/sum(ctab[2,])
precision

# compute recall = true positives / actual true
recall <- ctab[2,2]/sum(ctab[,2])
recall

# look at ROC curve
library(pROC)
roccurve <- roc(people$SleepOptimal ~ people$pred)
plot(roccurve)

# pull out just the AUC statistic
auc(roccurve)
```

### NULL MODEL for Logistic Regression

We didn't cover this in class, but to get an "intercept-only" model you use a formula in the form of `outcome ~ 1` which basically says model the `outcome` variable as a function of the intercept indicated by the `1`. This formula can be used for any generalized linear modeling approach (linear regression, logistic regression, Poisson regression, etc). You'll notice in running the code steps below that using the intercept only approach does no better than flipping a coin which you see for the ROC curve which is a straight line and the AUC is 0.5 (50/50 guessing does as well as this null model with no predictors). You always want the AUC to be >0.5 and as close to 1.0 as possible. AUCs >0.7 are ok but you really want AUCs >0.8 and >0.9 is even better.

```{r}
# NULL MODEL for Logistic Regression 
# is basically an intercept-only model with no predictors
logreg.null <- glm(SleepOptimal ~ 1, 
                   data=people, 
                   family=binomial(link="logit"))

summary(logreg.null)

people$pred.null <- predict(logreg.null, 
                       newdata=people, 
                       type="response")

# plot predicted probabilities
ggplot(people, 
       aes(x=pred.null, color=as.factor(SleepOptimal), 
           linetype=as.factor(SleepOptimal))) +
  geom_density() +
  ggtitle("Predicted Probability for Sleep Optimal - Null Model")

# look at ROC curve
#library(pROC)
roccurve <- roc(people$SleepOptimal ~ people$pred.null)
plot(roccurve)

# pull out just the AUC statistic
auc(roccurve)
```

### Try KNN to predict Sleep Optimal

```{r}
# for knn, rpart and randomForest, set SleepOptimal
# back to being a factor
people$SleepOptimal <- as.factor(people$SleepOptimal)

# Apply knn procedure to predict Diabetes
# use the knn procedure in the class package
library(class)

# Let's try different values of k to see how that affects performance
knn.1 <- knn(train = people, test = people, cl = people$SleepOptimal, k = 1)
knn.3 <- knn(train = people, test = people, cl = people$SleepOptimal, k = 3)
knn.5 <- knn(train = people, test = people, cl = people$SleepOptimal, k = 5)
knn.20 <- knn(train = people, test = people, cl = people$SleepOptimal, k = 20)

# see how well they classified
# Calculate the percent predicted correctly

100*sum(people$SleepOptimal == knn.1)/length(knn.1)
100*sum(people$SleepOptimal == knn.3)/length(knn.3)
100*sum(people$SleepOptimal == knn.5)/length(knn.5)
100*sum(people$SleepOptimal == knn.20)/length(knn.20)

#overall success
# Another way to look at success rate against increasing k

table(knn.1, people$SleepOptimal)
table(knn.3, people$SleepOptimal)
table(knn.5, people$SleepOptimal)
table(knn.20, people$SleepOptimal)
```

### (Version 1) The ensemble method - using the approach from lesson 10 with Age and BMI

The example here uses the same basic code we did in class for lesson 10. This looks at just Age and BMI and no other variables considered in the decision tree and random forest models. The plots also just consider Age and BMI.

```{r}
#library(mosaic)
# Create the grid
ages <- mosaic::range(~ Age, data = people)
bmis <- mosaic::range(~ BMI, data = people)
res <- 100
fake_grid <- expand.grid(
  Age = seq(from = ages[1], to = ages[2], length.out = res),
  BMI = seq(from = bmis[1], to = bmis[2], length.out = res))

#Get the overall proportion, p, of people with Sleep Trouble
p <- sum(people$SleepOptimal == 1)/length(people$SleepOptimal)
p

# Null model prediction
pred_null <- rep(p, nrow(fake_grid))

form <- as.formula("SleepOptimal ~ Age + BMI")

#library(rpart)
# Evaluate each model on each grid point
# For the decision tree
dmod_tree <- rpart(form, data = people, 
                   control = rpart.control(cp = 0.005, minbucket = 30))

# results summary
dmod_tree

# For the forest
set.seed(20371)
#dmod_forest <- rfsrc(form, data = people, 
#                     ntree = 201, mtry = 3)
# try with randomForest instead of randomForestSRC package
#library(randomForest)
dmod_forest <- randomForest(form, data = people, 
                     ntree = 201, mtry = 2)

# results summary
dmod_forest

# Now the predictions for tree and forest
# keep column 2
pred_tree <- predict(dmod_tree, newdata = fake_grid)[,2]
summary(pred_tree)

pred_forest <- predict(dmod_forest, newdata = fake_grid, type = "prob")[,2]
summary(pred_forest)

# K-nearest neighbor prediction
pred_knn <- people %>%
  select(Age, BMI) %>%
  knn(test=select(fake_grid, Age, BMI), cl = people$SleepOptimal, k=5) %>%
  as.numeric() - 1

```

Next, we want to build a dataframe with all of these predicted models, then `gather()` it into a long format.

```{r}
#library(tidyr)
# build the data frame
res <- fake_grid %>%
  mutate(
    "Null" = pred_null, 
    "Decision Tree" = pred_tree,
    "Random Forest" = pred_forest, 
    "K-nearest neighbor" = pred_knn) %>%
  gather(k="model", value = "y_hat", -Age, -BMI)

```

Next let's plot all of these

```{r}
ggplot(data = res, aes(x = Age, y = BMI)) +
  geom_tile(aes(fill=y_hat), color = NA) +
  geom_count(aes(color = SleepOptimal), alpha = 0.4, data = people) +
  scale_fill_gradient(low = "white", high = "blue") +
  scale_color_manual(values = c("gray", "gold")) +
  scale_size(range = c(0,2)) +
  scale_x_continuous(expand = c(0.02, 0)) +
  scale_y_continuous(expand = c(0.02, 0)) +
  facet_wrap(~model)

```

## An Analysis of `SleepHrsNight` as a Continuous/Numeric Outcome Variable

We could also analyze sleep hours per night using the original continuous/numeric data.

### Linear Regression

So, instead of running a logistic regression model for a binary/categorical outcome, we'll run a linear regression for `SleepHrsNight` as a numeric/continuous outcome. After running the model, the predictions are saved and then plotted against the original values. As you can see in the plot below, the model does not do a very good job. The adjusted R2 is only 0.04 and the predicted sleep times are very narrow (basically around the mean) and do not lie along a y=x reference line.

```{r}
# build dataset for these 10 variables and SleepHrsNight
people <- NHANES %>% 
  select(Age, Gender, MaritalStatus, Poverty, HomeOwn,
         BMI, Diabetes, HealthGen, Depressed, PhysActive,
         SleepHrsNight) 

# run summary
summary(people)

# Convert back to dataframe
people <- as.data.frame(people)
glimpse(people)

# Convert factors to numeric - the packages just seem to work better that way
people$Gender <- as.numeric(people$Gender)
people$MaritalStatus <- as.numeric(people$MaritalStatus)
people$HomeOwn <- as.numeric(people$HomeOwn)
people$Diabetes <- as.numeric(people$Diabetes)
people$HealthGen <- as.numeric(people$HealthGen)
people$Depressed <- as.numeric(people$Depressed)
people$PhysActive <- as.numeric(people$PhysActive)
people$SleepHrsNight <- as.numeric(people$SleepHrsNight)

summary(people)
dim(people)

# drop any cases/rows with missing data
# this step creates a complete cases dataset
people <- na.omit(people)
summary(people)
dim(people)

# model SleepTrouble by rest of variables in people dataset
fmla <- "SleepHrsNight ~ ."

lm1 <- lm(fmla, data=people)

summary(lm1)

# this time lm1 is of class "lm"
# so we are really running predict.lm() below
people$pred <- predict(lm1, 
                       newdata=people, 
                       type="response")

# plot predicted sleep times
# against the original sleep times
# add y=x reference line
ggplot(people, 
       aes(x=SleepHrsNight, y=pred)) +
  geom_point() + 
  geom_abline(slope=1, intercept=0) +
  ggtitle("Predicted Sleep Times vs Actual Sleep Times")
```

### NULL Model for Regression

The NULL model for linear regression uses the same approach as above - basically and intercept-only model which is $Y = \beta_0 + \epsilon$ where basically the outcome `Y` is estimated by the grand mean.

```{r}
lm1.null <- lm("SleepHrsNight ~ 1", data=people)

summary(lm1.null)

people$pred.null <- predict(lm1.null, 
                       newdata=people, 
                       type="response")
```

We can plot the null model predictions against the original sleep times.

```{r}
# plot predicted sleep times
# against the original sleep times
# add y=x reference line
ggplot(people, 
       aes(x=SleepHrsNight, y=pred.null)) +
  geom_point() + 
  geom_abline(slope=1, intercept=0) +
  ggtitle("Null Model Predicted Sleep Times vs Actual Sleep Times")
```

We can compare these 2 models using the `anova()` command. While the original linear model did not do very well, it does do a better job predicting sleep times than the null model did.

```{r}
knitr::kable(anova(lm1.null, lm1))
```

### KNN Classifications

```{r}
# Let's try different values of k to see how that affects performance
knn.1 <- knn(train = people, test = people, cl = people$SleepHrsNight, k = 1)
knn.3 <- knn(train = people, test = people, cl = people$SleepHrsNight, k = 3)
knn.5 <- knn(train = people, test = people, cl = people$SleepHrsNight, k = 5)
knn.20 <- knn(train = people, test = people, cl = people$SleepHrsNight, k = 20)

# see how well they classified
# Calculate the percent predicted correctly

100*sum(people$SleepHrsNight == knn.1)/length(knn.1)
100*sum(people$SleepHrsNight == knn.3)/length(knn.3)
100*sum(people$SleepHrsNight == knn.5)/length(knn.5)
100*sum(people$SleepHrsNight == knn.20)/length(knn.20)

#overall success
# Another way to look at success rate against increasing k

table(knn.1, people$SleepHrsNight)
table(knn.3, people$SleepHrsNight)
table(knn.5, people$SleepHrsNight)
table(knn.20, people$SleepHrsNight)
```

```{r}
form <- as.formula("SleepHrsNight ~ Age + Gender + MaritalStatus + 
                   Poverty + HomeOwn + BMI + Diabetes + HealthGen + 
                   Depressed + PhysActive")

#library(rpart)
# Evaluate each model on each grid point
# For the decision tree
dmod_tree <- rpart(form, data = people, 
                   control = rpart.control(cp = 0.005, minbucket = 30))

# results summary
dmod_tree

# draw the tree - see example
# in the help at help(plot.rpart)
par(xpd = TRUE)
plot(dmod_tree, compress = TRUE)
text(dmod_tree, use.n = TRUE)
# Age and HealthGen were important for predicting
# sleep times

# For the forest
set.seed(20371)
#dmod_forest <- rfsrc(form, data = people, 
#                     ntree = 201, mtry = 3)
# try with randomForest instead of randomForestSRC package
#library(randomForest)
dmod_forest <- randomForest(form, data = people, 
                     ntree = 201, mtry = 2)

# results summary
dmod_forest
varImpPlot(dmod_forest)
# you'll notice that BMI and Age are at the top
# of this Variable Important plot
# also near the top is the Poverty level

# Now the predictions for tree and forest
# just compute prediction from original data for now

# **NOTE:** there is only 1 column of output
# from predict() since the outcome is continuous and
# NOT a factor of different "classes"
pred_tree <- predict(dmod_tree)
summary(pred_tree)

pred_forest <- predict(dmod_forest)
summary(pred_forest)
```

Let's make a plot of each of the predictions against the original values. **NOTE:** I left off the predictions from the KNN model in the ensemble plot below.

```{r}
# define multiplot function
# see http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
# for how to create this function
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

# make the plots
people$pred_tree <- pred_tree
people$pred_forest <- pred_forest
p1 <- ggplot(people, aes(x=SleepHrsNight, y = value, color = value)) + 
    geom_point(aes(y = pred.null),
               colour="blue") + 
    geom_abline(slope=1, intercept=0) +
    ggtitle("NULL Model")

p2 <- ggplot(people, aes(x=SleepHrsNight, y = value, color = value)) + 
    geom_point(aes(y = pred),
               colour="red") +
    geom_abline(slope=1, intercept=0) +
  ggtitle("Linear Model")

p3 <- ggplot(people, aes(x=SleepHrsNight, y = value, color = value)) + 
    geom_point(aes(y = pred_tree),
               colour="green") +
    geom_abline(slope=1, intercept=0) +
  ggtitle("Decision Tree")

p4 <- ggplot(people, aes(x=SleepHrsNight, y = value, color = value)) + 
    geom_point(aes(y = pred_forest),
               colour="purple") +
    geom_abline(slope=1, intercept=0) +
  ggtitle("Random Forest")

multiplot(p1, p2, p3, p4, cols=2)
```

Of these 4, the Random Forest model results seem to do the best job predicting sleep times.

